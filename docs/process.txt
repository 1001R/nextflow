###########
Processes
###########

In Nextflow a `process` is the basic processing `primitive` to execute a user script.

The process definition starts with keyword the ``process``, followed by process name and finally the process `body`
delimited by curly brackets. The process body must contain a string which represents the command or, more generally,
a script that is executed by it. A basic process looks like the following example::

  process sayHello {

      "echo 'Hello world!' > file"

  }


more specifically a process may contain five definition blocks, respectively: directives,
inputs, outputs, shares and finally the process script. The syntax is defined as follows:

::

  process < name > {

     [ directives ]

     input:
      < process inputs >

     output:
      < process outputs >

     share:
      < shared input/output >

     [script:|exec:]
     < user script to be executed >

  }



Script
=======

The `script` block is a string statement that defines the command that is executed by the process to carry out its task.

A process contains one and only one script block, and it must be the last statement when the process contains
input, output and share declarations.

The entered string is executed as a `BASH <http://en.wikipedia.org/wiki/Bash_(Unix_shell)>`_ script in the
`host` system. It can be any command, script or combination of them, that you would normally use in terminal shell
or in a common BASH script.

The only limitation to the commands that can be used in the script statement is given by the availability of those
programs in the target execution system.


The script block can be a simple string or multi-line string. The latter simplifies the writing of non trivial scripts
composed by multiple commands spanning over multiple lines. For example::

    process doMoreThings {

      """
      blastp -db $db -query query.fa -outfmt 6 > blast_result
      cat blast_result | head -n 10 | cut -f 2 > top_hits
      blastdbcmd -db $db -entry_batch top_hits > sequences
      """

    }

As explained in the script tutorial section, strings can be defined by using a single-quote
or a double-quote, and multi-line strings are defined by three single-quote or three double-quote characters.

There is a subtle but important difference between them. Like in BASH, strings delimited by a ``"`` character support
variable substitutions, while strings delimited by ``'`` do not.

In the above code fragment the ``$db`` variable is replaced by the actual value defined somewhere in the
pipeline script.

.. warning:: Since Nextflow uses the same BASH syntax for variable substitutions in strings, you need to manage them
  carefully depending on if you want to evaluate a variable in the Nextflow context - or - in the BASH environment execution.

When you need to access a system environment variable  in your script you have two options. The first choice is as
easy as defining your script block by using a single-quote string. For example::

    process printPath {

       '''
       echo The path is: $PATH
       '''

    }

The drawback of this solution is that you will not able to access variables defined in the pipeline script context,
in your script block.

To fix this, define your script by using a double-quote string and `escape` the system environment variables by
prefixing them with a back-slash ``\`` character, as shown in the following example::


    process doOtherThings {

      """
      blastp -db \$DB -query query.fa -outfmt 6 > blast_result
      cat blast_result | head -n $MAX | cut -f 2 > top_hits
      blastdbcmd -db \$DB -entry_batch top_hits > sequences
      """

    }

In this example the ``$MAX`` variable has to be defined somewhere before, in the pipeline script.
`Nextflow` replaces it with the actual value before executing the script. Instead, the ``$DB`` variable
must exist in the script execution environment and the BASH interpreter will replace it with the actual value.


Scripts `Ã  la carte`
--------------------

The process script is interpreted by Nextflow as a BASH script by default, but you are not limited to it.

You can use your favourite scripting language (e.g. Perl, Python, Ruby, R, etc), or even mix them in the same pipeline.

A pipeline may be composed by processes that execute very different tasks. Using `Nextflow` you can choose the scripting
language that better fits the task carried out by a specified process. For example for some processes `R` could be
more useful than `Perl`, in other you may need to use `Python` because it provides better access to a library or an API, etc.

To use a scripting other than BASH, simply start your process script with the corresponding
`shebang <http://en.wikipedia.org/wiki/Shebang_(Unix)>`_ declaration. For example::

    process perlStuff {

        """
        #!/usr/bin/perl

        print 'Hi there!' . '\n';
        """

    }

    process pyStuff {

        """
        #!/usr/bin/python

        x = 'Hello'
        y = 'world!'
        print "%s - %s" % (x,y)
        """

    }


.. tip:: Since the actual location of the interpreter binary file can change across platforms, to make your scripts
   more portable it is wise to use the ``env`` shell command followed by the interpreter's name, instead of the absolute
   path of it. Thus, the `shebang` declaration for a Perl script, for example,
   would look like: ``#!/usr/bin/env perl`` instead of the one in the above pipeline fragment.


Conditional scripts
-------------------

Complex process scripts may need to evaluate conditions on the input parameters or use traditional flow control
statements (i.e. ``if``, ``switch``, etc) in order to execute specific script commands, depending on the current
inputs configuration.

Process scripts can contain conditional statements by simply prefixing the script block with the keyword ``script:``.
By doing that the interpreter will evaluate all the following statements as a code block that must return the
script string to be executed. It's much easier to use than to explain, for example::


    seq_to_align = ...
    mode = 'tcoffee'

    process align {
        input:
        file seq_to_aln from sequences

        script:
        if( mode == 'tcoffee' )
            """
            t_coffee -in $seq_to_aln > out_file
            """

        else if( mode == 'mafft' )
            """
            mafft --anysymbol --parttree --quiet $seq_to_aln > out_file
            """

        else if( mode == 'clustalo' )
            """
            clustalo -i $seq_to_aln -o out_file
            """

        else
            error "Invalid alignment mode: ${mode}"

    }


In the above example the process will execute the script fragment depending on the value of the ``mode`` parameter.
By default it will execute the ``tcoffee`` command, changing the ``mode`` variable to ``mafft`` or ``clustalo`` value,
the other branches will be executed.


Native execution
------------------

Nextflow processes can execute native code other than system scripts as shown in the previous paragraphs.

This means that instead of specifying the process command to be executed as a string script, you can
define it by providing one or more language statements, as you would do in the rest of the pipeline script.
Simply starting the script definition block with the ``exec:`` keyword, for example::

    x = Channel.from( 'a', 'b', 'c')

    process simpleSum {
        input:
        val x

        exec:
        println "Hello Mr. $x"
    }

Will display::

    Hello Mr. b
    Hello Mr. a
    Hello Mr. c


.. warning:: Native processes execution is an incubating feature and has the following limitations:
    they can only be executed by using a `local` executor and they cannot be used when the `merge`
    feature is specified.


Inputs
=======

Nextflow processes are isolated from each other but can communicate between themselves sending values through channels.

The `input` block defines which channels the process is expecting to receive inputs data from. You can only define one
input block at a time and it must contain one or more inputs declarations.

The input block follows the syntax shown below::

    input:
      <input classifier> <input name> [from channel] [attributes]


Input definitions start with an input `classifier` and the input `name`, followed by the keyword ``from`` and
the actual channel over which inputs are received. Finally some input optional attributes can be specified.

.. note:: When the input name is the same as the channel name, the ``from`` part of the declaration can be omitted.

The input classifier declares the `type` of data to be received. This information is used by Nextflow to apply the
semantic rules associated to each classifier and handle it properly depending on the target execution platform
(grid, cloud, etc).

The classifiers available are the ones listed in the following table:

=========== =============
Classifier  Semantic
=========== =============
val         Lets you access the received input value by its name in the process script.
env         Lets you use the received value to set an environment variable named
            as the specified input name.
file        Lets you handle the received value as a file, staging it properly in the execution context.
stdin       Lets you forward the received value to the process `stdin` special file.
set         Lets you handle a group of input values having one of the above classifiers.
each        Lets you execute the process for each entry in the input collection.
=========== =============


Input of generic values
-------------------------

The ``val`` classifier allows you to receive data of any type as input. It can be accessed in the process script
by using the specified input name, as shown in the following example::

    num = Channel.from( 1, 2, 3 )

    process basicExample {
      input:
      val x from num

      "echo process job $x"

    }


In the above example the process is executed three times, each time a value is received from the channel ``num``
and used to process the script. Thus, it results in an output similar to the one shown below::

    process job 3
    process job 1
    process job 2

.. note:: The `channel` guarantees that items are delivered in the same order as they have been sent - but -
  since the process is executed in a parallel manner, there is no guarantee that they are processed in the
  same order as they are received. In fact, in the above example, value ``3`` is processed before the others.


When the ``val`` has the same name as the channel from where the data is received, the ``from`` part can be omitted.
Thus the above example can be written as shown below::

    num = Channel.from( 1, 2, 3 )

    process basicExample {
      input:
      val num

      "echo process job $num"

    }


Input of files
-----------------

The ``file`` classifier allows you to receive a value as a file in the process execution context. This means that
Nextflow will stage it in the process execution directory, and you can access it in the script by using the name
specified in the input declaration. For example::

    proteins = Channel.files( '/some/path/*.fa' )

    process blastThemAll {
      input:
      file query_file from proteins

      "blastp -query ${query_file} -db nr"

    }

In the above example all the files ending with the suffix ``.fa`` are sent over the channel ``proteins``.
Then, these files are received by the process which will execute a `BLAST` query on each of them.

When the file input name is the same as the channel name, the ``from`` part of the input declaration can be omitted.
Thus, the above example could be written as shown below::

    proteins = Channel.files( '/some/path/*.fa' )

    process blastThemAll {
      input:
      file proteins

      "blastp -query $proteins -db nr"

    }


It's worth noting that in the above examples, the name of the file in the file-system is not touched, you can
access the file even without knowing its name because you can reference it in the process script by using the
variable whose name is specified in the input file parameter declaration.

There may be cases where your task needs to use a file whose name is fixed, it does not have to change along
with the actual provided file. In this case you can specify its name by specifying the ``name`` attribute in the
input file parameter declaration, as shown in the following example::

    input:
        file query_file name 'query.fa' from proteins


Or alternatively using a shorter syntax::

    input:
        file 'query.fa' from proteins


Using this, the previous example can be re-written as shown below::

    proteins = Channel.files( '/some/path/*.fa' )

    process blastThemAll {
      input:
      file 'query.fa' from proteins

      "blastp -query query.fa -db nr"

    }


What happens in this example is that each file, that the process receives, is staged with the name ``query.fa``
in a different execution context (i.e. the folder where the job is executed) and an independent process
execution is launched.

.. tip:: This allows you to execute the process command various time without worrying the files names changing.
  In other words, `Nextflow` helps you write pipeline tasks that are self-contained and decoupled by the execution
  environment. This is also the reason why you should avoid whenever possible to use absolute or relative paths
  referencing files in your pipeline processes.



.. TODO describe that file can handle channels containing any data type not only file


Multiple files
----------------------

A process can declare as input file a channel that emits a collection of values, instead of a simple value.

In this case, the script variable defined by the input file parameter will hold a list of files. You can
use it as shown before, referring to all the files in the list, or by accessing a specific entry using the
usual square brackets notation.

When a target file name is defined in the input parameter and a collection of files is received by the process,
the file name will be appended by a numerical suffix representing its ordinal position in the list. For example::

    fasta = Channel.files( "/some/path/*.fa" ).buffer(count:3)

    process blastThemAll {
        input:
        file 'seq' from fasta

        "echo seq*"

    }

Will output::

    seq1 seq2 seq3
    seq1 seq2 seq3
    ...

The target input file name can contain the ``*`` and ``?`` wildcards, that can be used
to control the name of staged files. The following table shows how the wildcards are
replaced depending on the cardinality of the received input collection.

============ ============== ==================================================
Cardinality   Name pattern     Staged file names
============ ============== ==================================================
 1           ``file*.ext``   ``file.ext``
 1           ``file?.ext``   ``file1.ext``
 1           ``file??.ext``  ``file01.ext``
 many        ``file*.ext``   ``file1.ext``, ``file2.ext``, ``file3.ext``, ..
 many        ``file?.ext``   ``file1.ext``, ``file2.ext``, ``file3.ext``, ..
 many        ``file??.ext``  ``file01.ext``, ``file02.ext``, ``file03.ext``, ..
============ ============== ==================================================

The following fragment shows how a wildcard can be used in the input file declaration::


    fasta = Channel.files( "/some/path/*.fa" ).buffer(count:3)

    process blastThemAll {
        input:
        file 'seq?.fa' from fasta

        "cat seq1.fa seq2.fa seq3.fa"

    }



Input of type 'stdin'
-----------------------

The ``stdin`` input classifier allows you the forwarding of the value received from a channel to the
`standard input <http://en.wikipedia.org/wiki/Standard_streams#Standard_input_.28stdin.29>`_
of the command executed by the process. For example::

    str = Channel.from('hello', 'hola', 'bonjour', 'ciao').map { it+'\n' }

    process printAll {
       input:
       stdin str

       """
       cat -
       """

    }

It will output::

    hola
    bonjour
    ciao
    hello




Input of type 'env'
---------------------

The ``env`` classifier allows you to define an environment variable in the process execution context based
on the value received from the channel. For example::

    str = Channel.from('hello', 'hola', 'bonjour', 'ciao')

    process printEnv {

        input:
        env HELLO from str

        '''
        echo $HELLO world!
        '''

    }

::

    hello world!
    ciao world!
    bonjour world!
    hola world!



Input of type 'set'
--------------------

The ``set`` classifier allows you to group multiple parameters in a single parameter definition. It can be useful
when a process receives, in input, tuples of values that need to be handled separately. Each element in the tuple
is associated to a corresponding element with the ``set`` definition. For example::

     tuple = Channel.from( [1, 'alpha'], [2, 'beta'], [3, 'delta'] )

     process setExample {
         input:
         set val(x), file('latin.txt')  from tuple

         """
         echo Processing $x
         cat - latin.txt > copy
         """

     }


In the above example the ``set`` parameter is used to define the value ``x`` and the file ``latin.txt``,
which will receive a value from the same channel.

In the ``set`` declaration items can be defined by using the following classifiers: ``val``, ``env``, ``file`` and ``stdin``.

A shorter notation can be used by applying the following substitution rules:

============== =======
long            short
============== =======
val(x)          x
file(x)         (not supported)
file('name')    'name'
file(x:'name')  x:'name'
stdin           '-'
env(x)          (not supported)
============== =======

Thus the previous example could be rewritten as follows::

      tuple = Channel.from( [1, 'alpha'], [2, 'beta'], [3, 'delta'] )

      process setExample {
          input:
          set x, 'latin.txt' from tuple

          """
          echo Processing $x
          cat - latin.txt > copy
          """

      }



Input repeaters
----------------

The ``each`` classifier allows you to repeat the execution of a process for each item in a collection,
every time new data is received. For example::

  sequences = Channel.files('*.fa')
  methods = ['regular', 'expresso', 'psicoffee']

  process alignSequences {
    input:
    file seq from sequences
    each mode from methods

    """
    t_coffee -in $seq -mode $mode > result
    """

  }


In the above example every time a file of sequences is received as input by the process,
it executes three T-coffee tasks, using a different value for the ``mode`` parameter.

This is useful when you need to `repeat` the same task for a given set of parameters.

.. note:: When multiple repeaters are declared, the process is executed for each *combination* them.

Take in consideration the following example. The process declares, in input, a channel receiving a
generic ``shape`` of values. Each time a new shape value is received, it `draws` it
in two different colors and three different sizes::

    shapes = Channel.from('circle','square', 'triangle' .. )

    process combine {
      input:
      val shape from shapes
      each color from 'red','blue'
      each size from 1,2

      "echo draw $shape $color with size: $size"

    }

Will output::

    draw circle red with size: 1
    draw circle red with size: 2
    draw circle red with size: 3
    draw circle blue with size: 1
    draw circle blue with size: 2
    draw circle blue with size: 3
    draw square red with size: 1
    draw square red with size: 2
    draw square red with size: 3
    draw square blue with size: 1
    draw square blue with size: 2
    draw square blue with size: 3
    draw triangle red with size: 1
    draw triangle red with size: 2
    draw triangle red with size: 3
    draw triangle blue with size: 1
    draw triangle blue with size: 2
    draw triangle blue with size: 3
    ..


Outputs
========

The `output` declaration block allows to define the channels used by the process to send out the results produced.

It can be defined at most one output block and it can contain one or more outputs declarations.
The output block follows the syntax shown below::

    output:
      <output classifier> <output name> [into <channel>] [attribute [,..]]

Output definitions start by an output `classifier` and the output `name`, followed by the keyword ``into`` and
the actual channel over which outputs are sent. Finally some optional attributes can be specified.

.. note:: When the output name is the same as the channel name, the ``into`` part of the declaration can be omitted.


.. TODO the channel is implicitly create if does not exist

The provided classifiers are:

- *val*: handle data of any type;
- *file*: the output is managed as file to be staged in the process context;
- *stdout*: the received data is redirected to the process `stdout` special file;


The classifiers that can be used in the output declaration block are the ones listed in the following table:

=========== =============
Classifier  Semantic
=========== =============
val         Sends variable's with the name specified over the output channel.
file        Sends a file produced by the process with the name specified over the output channel.
stdout      Sends the executed process `stdout` over the output channel.
set         Lets to send multiple values over the same output channel.
=========== =============


Output values
-------------------------

The ``val`` classifier allows to output a `value` defined in the script context. In a common usage scenario,
this is a value which has been defined in the `input` declaration block, as shown in the following example::

   methods = ['prot','dna', 'rna']

   process anyValue {
     input:
     val x from methods

     output:
     val x into receiver

     "echo $x > file"

   }

   receiver.subscribe { println "Received: $it" }


Output files
-----------------

The ``file`` classifier allows to output one or more files, produced by the process, over the specified channel.
For example::


    process randomNum {

       output:
       file 'result' into numbers

       '''
       echo $RANDOM > result
       '''

    }

    numbers.subscribe { println "Received: " + it.text }


In the above example the process, when executed, creates a file named ``result`` containing a random number.
Since a file parameter using the same name is declared between the outputs, when the task is completed that
file is sent over the ``numbers`` channel. A downstream `process` declaring the same channel as `input` will
be able to receive it.

.. note:: If the channel specified as output has not been previously declared in the pipeline script, it
  will implicitly created by the output declaration itself.


.. TODO explain Path object

Multiple files
-----------------

When declaring an output file it is possible to specify its name using the usual Linux wildcards characters ``?`` and ``*``.
This allows to output all the files matching the specified file name pattern as single item. For example::

    process splitLetters {

        output:
        file 'chunk_*' into letters

        '''
        printf 'Hola' | split -b 1 - chunk_
        '''
    }

    letters.subscribe { println it *.text }

::

    [H, o, l, a]


.. TODO Advanced file output



Output 'stdout' special file
-------------------------------

The ``stdout`` classifier allows to `capture` the `stdout` output of the executed process and send it over
the channel specified in the output parameter declaration. For example::

    process echoSomething {
        output:
        stdout channel

        "echo Hello world!"

    }

    channel.subscribe { print "I say..  $it" }





Output 'set' of values
--------------------------

The ``set`` classifier allows to send multiple values into a single channel. This feature is useful
when you need to `together` the result of multiple execution of the same process, as shown in the following
example::

    query = Channel.files '*.fa'
    specie = Channel.from 'human', 'cow', 'horse'

    process blast {

    input:
        val specie
        file query

    output:
        set val(specie), file('result') into blastOuts


    "blast -db nr -query $query" > result

    }


In the above example a `BLAST` task is execute for each pair of ``specie`` and ``query`` that aer received.
When the task completes a tuple containing the current value of ``specie`` and the file ``result`` is set over
the ``blastOuts`` channel.


A ``set`` declaration can contains any combination of the following classifiers, previously described:
 ``val``, ``file`` and ``stdout``.



Directives
==========

In the `directive` compartment can be specified configuration options which will affect the execution of the current
process. They have the following syntax::

    name value [, value2 [,..]]

Below are listed directives that can be applied to any process.


cacheable
---------

Whenever the process outputs have to caches across multiple pipeline executions. It is ``true`` by default, set to ``false``
to disable caching feature for this process. For example::

  process noCacheThis {

    cache false

    """
    command not to be cached
    """
  }


echo
-----

Whenever print out the process standard output the current execution console. It is ``false`` by default, set to ``true``
if you need to show the process output.


errorStrategy
--------------

The ``errorStrategy`` directive defines how an error condition is managed by the process. By default when an error status
is returned by the execute script, the process stops immediately that in turns forces cause the entire pipeline to stops.

When using the directive ``errorStrategy 'ignore'`` the process do not stop on error condition and it just reports a message
to notify the error event.

For example::

    process ignoreAnyError {
      errorStrategy 'ignore'

      "command to be executed"
    }

.. note:: By definition a command script is failing when it terminates with a non-zero exit status. To change this behavior
  see `validExitStatus`_.


executor
--------

The ``executor`` directive allows to specify the underlying system on which the process is actually executed. The following
values can be used:

- ``local``: The process is executed in the local computer where `Nextflow` is launched.

- ``sge``: the process is executed using a Sun Grid Engine / `Open Grid Engine <http://gridscheduler.sourceforge.net/>`_.

- ``lsf``: The process is executed via the `Platform LSF <http://en.wikipedia.org/wiki/Platform_LSF>`_ job scheduler.

- ``slurm``: The process is executed via the SLURM job scheduler.

- ``dnanexus``: The process is executed by submitting it to a `DNAnexus <http://www.dnanexus.com/>`_ cloud.



validExitStatus
-------------------

 (default: ``0``)


..
    clusterOptions
    ---------------

    queue
    -----

    (sge,lsf)

    maxDuration
    ------------
    (sge,slurm)

    maxMemory
    ---------
    (sge)




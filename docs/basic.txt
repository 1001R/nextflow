###############
Basic concepts
###############


`Nextflow` is a reactive workflow framework and a programming `DSL <http://en.wikipedia.org/wiki/Domain-specific_language>`_
that ease the writing of computational pipelines with complex data.

It is designed around the idea that the Linux platform is the lingua franca of data science, since it provides many
simple command line and scripting tools, which by themselves are powerful, but when chained together facilitate complex
data manipulations.

`Nextflow` extends this model adding to it the ability to define complex processes interactions and an high-level
parallel computational environment based on the `dataflow` programming model.

Processes and channels
----------------------

In practice, Nextflow pipeline script is defined by composing many different processes.
Each process can be written in any scripting language that can be executed by the Linux platform (BASH, Perl, Ruby, Python, etc).

Processes are executed independently and are isolated each others i.e. they do not share a common (writable) state.
The only way they can communicate is by using asynchronous FIFO queues, called `channels` in the Nextflow model.

Any process can define one or more channels as `input` and `output`. The processes interaction and ultimately the
pipeline execution flow is implicitly defined by these inputs and outputs declaration. For example::

    params.query = "$HOME/sample.fa"
    params.db = "$HOME/tools/blast-db/pdb/pdb"

    db = file(params.db)
    query = file(params.query)

    process blastSearch {
        input:
        file query

        output:
        file top_hits

        """
        blastp -db $db -query $query -outfmt 6 > blast_result
        cat blast_result | head -n 10 | cut -f 2 > top_hits
        """
    }


    process extractTopHits {
        input:
        file top_hits

        output:
        file sequences

        """
        blastdbcmd -db ${db} -entry_batch $top_hits > sequences
        """
    }



In the above example are defined two processes, the execution order is not set by the fact the ``blastSearch`` process comes
before the ``extractTopHits`` in the script, it could be written also in the other way around, but because the latter defines
the channel ``top_hits`` in its inputs declaration while the ``blastSearch`` process has the channel define as in the outputs
declaration.


Execution abstraction
---------------------

While a process define `what` command or script has to be executed, the `executor` has the role to determine `how` that
script is actually run in the target system.

If not specified otherwise processes are executed in the local computer. Local executor is very useful for pipeline
development and test, but for real world bioinformatics pipelines HPC or cloud platform may be required.

In other words, `Nextflow` provides an abstraction between the pipeline functional logic and the underlying execution system.
Thus it is possible to write a pipeline once and have it running on your computer, a grid platform or the cloud seamlessly,
without modifying it, by simply defined the target execution platform in the configuration file.

Alternatively the following HPC and cloud platforms are supported:

* `Sun/Open Grid Engine <http://gridscheduler.sourceforge.net/>`_
* `Platform LSF <http://www.ibm.com/systems/technicalcomputing/platformcomputing/products/lsf/>`_
* `SLURM <https://computing.llnl.gov/linux/slurm/>`_
* `DNAnexus <http://www.dnanexus.com>`_



Scripting language
------------------

Beside the fact that `Nextflow` is designed to be used with a minimal learning curve, without having having to study
a new programming language and using your current skill, it also provides powerful DSL scripting language.

The Nextflow scripting is a super-set of the `Groovy programming language <http://en.wikipedia.org/wiki/Groovy_(programming_language)>`_
which in turn is a super-set of the Java programming, thus if you have some knowledge with those, or even only some confidence
with the `C/C++` languages syntax, you will comfortable with it.







Running pipeline
----------------


Pipeline parameters
--------------------


Configuration options
---------------------



